{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textFunctions import *\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from nltk import FreqDist\n",
    "import lda\n",
    "from ast import literal_eval\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "from __future__ import print_function\n",
    "import funcy as fp\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.utils import tokenize\n",
    "# from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textFunctions import *\n",
    "pd.set_option('display.max_rows', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>The Discourse on the Arousing of Mindfulness d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Mindfulness is a process, an event and an aris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>For instance in association with Right Underst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book                                               text\n",
       "0       The Discourse on the Arousing of Mindfulness d...\n",
       "1                                                      \\n\n",
       "2       Mindfulness is a process, an event and an aris...\n",
       "3                                                      \\n\n",
       "4       For instance in association with Right Underst..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = glob.glob('./texts/*.txt')\n",
    "d = list()\n",
    "for book_file in books:\n",
    "    with open(book_file, encoding='utf-8') as f:\n",
    "        book = os.path.basename(book_file.split('.')[0])\n",
    "        d.append(pd.DataFrame({'book': book, 'text': f.readlines()}))\n",
    "doc = pd.concat(d)\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sati=doc[doc['text'] != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "suttas=pd.read_csv('./texts/suttas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "suttas.rename(columns={'Unnamed: 0':'book'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=[sati,suttas]\n",
    "doc=pd.concat(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)==len(sati)+len(suttas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>The Discourse on the Arousing of Mindfulness d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Mindfulness is a process, an event and an aris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>For instance in association with Right Underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>But the intuitive or rational role does not pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Mindfulness as memory is indicated by such ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book                                               text\n",
       "0       The Discourse on the Arousing of Mindfulness d...\n",
       "2       Mindfulness is a process, an event and an aris...\n",
       "4       For instance in association with Right Underst...\n",
       "6       But the intuitive or rational role does not pr...\n",
       "8       Mindfulness as memory is indicated by such ter..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['token'] = doc['text'].astype(str).apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>The Discourse on the Arousing of Mindfulness d...</td>\n",
       "      <td>[[The, Discourse, on, the, Arousing, of, Mindf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Mindfulness is a process, an event and an aris...</td>\n",
       "      <td>[[Mindfulness, is, a, process,, an, event, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>For instance in association with Right Underst...</td>\n",
       "      <td>[[For, instance, in, association, with, Right,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>But the intuitive or rational role does not pr...</td>\n",
       "      <td>[[But, the, intuitive, or, rational, role, doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Mindfulness as memory is indicated by such ter...</td>\n",
       "      <td>[[Mindfulness, as, memory, is, indicated, by, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  book                                               text  \\\n",
       "0       The Discourse on the Arousing of Mindfulness d...   \n",
       "2       Mindfulness is a process, an event and an aris...   \n",
       "4       For instance in association with Right Underst...   \n",
       "6       But the intuitive or rational role does not pr...   \n",
       "8       Mindfulness as memory is indicated by such ter...   \n",
       "\n",
       "                                               token  \n",
       "0  [[The, Discourse, on, the, Arousing, of, Mindf...  \n",
       "2  [[Mindfulness, is, a, process,, an, event, and...  \n",
       "4  [[For, instance, in, association, with, Right,...  \n",
       "6  [[But, the, intuitive, or, rational, role, doe...  \n",
       "8  [[Mindfulness, as, memory, is, indicated, by, ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used(art, ngrams=1, mod=None, month='', site=''):\n",
    "    '''Returns dict with counts, and the number of articles in a given dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - art: dataframe with articles\n",
    "    - ngrams: n of n-grams (default: 1, words)\n",
    "    - mod: dir to save files with articles (None if no saving necessary)\n",
    "    - month: month for filename\n",
    "    - site: site for filename\n",
    "    '''\n",
    "    word_dict = {} # save to it later\n",
    "    print(art.shape[0], month, site) # number of articles\n",
    "    if art.shape[0] == 0:\n",
    "        return {}, 0 # we don't divide by it later; continue\n",
    "    art['text'].fillna('', inplace=True) # get rid of nans, just in case. There should be none\n",
    "    # and there are none in articles, but reddit doesn't work without this line\n",
    "    time_start = datetime.now()\n",
    "    art_sh = art[['text']].copy() # we don't need more columns\n",
    "    del art # get rid of original df\n",
    "    gc.collect() # clean memory, save RAM\n",
    "    div = art_sh.shape[0]\n",
    "    print(art_sh.shape[0]) # number of articles without nans\n",
    "    \n",
    "    # clean texts a bit using regexp\n",
    "    # later we split by space (in function imported from textFunctions), so it's necessary\n",
    "    art_sh['text'] = art_sh['text'].apply(lambda x: re.sub('\\n', ' ', x))\n",
    "    art_sh['text'] = art_sh['text'].apply(lambda x: re.sub('\\t', ' ', x))\n",
    "\n",
    "    # tokenize\n",
    "    bigrams = False\n",
    "    if ngrams == 2:\n",
    "        bigrams = True\n",
    "    art_sh['text'] = art_sh['text'].apply(lambda x: tokenize(x, bigrams=bigrams, words=True, lang='english'))\n",
    "\n",
    "    # cleaning, quite slow, but necessary\n",
    "    # this function is actually quite minimal to save time, if you need something else, you may want more preprocessing\n",
    "    art_sh['text'] = art_sh['text'].apply(lambda x: cleaning(x, removePunctuation=True, removeNumbers=True, toLower=True, removeSpaces=True, stopwords=True, decodeDiacritics=False, shortwords=True)) \n",
    "\n",
    "    # frequency\n",
    "    fdist = FreqDist([i for sublist in art_sh['text'].tolist() for i in sublist])\n",
    "    x = dict(fdist)\n",
    "    del fdist\n",
    "    gc.collect()\n",
    "    \n",
    "    # we may need chunks later\n",
    "    # word_dict = {k: x.get(k, 0) + word_dict.get(k, 0) for k in set(x) | set(word_dict)}\n",
    "    \n",
    "    # save tokenized articles\n",
    "    if mod is not None:\n",
    "        art_sh[['text']].to_csv(mod + '' + str(ngrams) + '.csv')\n",
    "\n",
    "    print(datetime.now() - time_start)\n",
    "    del art_sh\n",
    "    gc.collect()\n",
    "    return x, div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712  \n",
      "3712\n",
      "0:00:19.356252\n",
      "(19466, 2)\n",
      "3712  \n",
      "3712\n",
      "0:00:23.348895\n",
      "(173210, 2)\n"
     ]
    }
   ],
   "source": [
    "res = './res/sutta/'\n",
    "for n in [1,2]:\n",
    "    word_dfs_all = pd.DataFrame()\n",
    "    # call the function defined above\n",
    "    word_dict, div = most_used(doc, ngrams=n, mod=res)\n",
    "            # convert result to df\n",
    "    if div == 0:\n",
    "        continue # avoid division by 0 and assigning column names to empty dataframes\n",
    "    word_df = pd.DataFrame.from_dict(word_dict, orient='index')\n",
    "    del word_dict\n",
    "    gc.collect()\n",
    "    # we need counts and frequencies\n",
    "    word_df.columns = ['count']\n",
    "    word_df['freq'] = word_df['count'] / div\n",
    "    word_dfs_all = word_df.copy()\n",
    "    print(word_dfs_all.shape)\n",
    "    del word_df\n",
    "    gc.collect()\n",
    "    word_dfs_all.to_csv(res + 'freq_' + str(n) + '-all.csv')\n",
    "    del word_dfs_all # we won't need it later\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=pd.read_csv('./res/sutta/freq_1-all.csv')\n",
    "freq.rename(columns={'Unnamed: 0': 'word'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.sort_values('count',ascending=False).to_csv('sutta_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "insight='''insight\n",
    "mindfulness\n",
    "mental\n",
    "feeling\n",
    "mind\n",
    "consciousness\n",
    "contemplating\n",
    "comprehension\n",
    "enlightenment\n",
    "reflection\n",
    "knowledge\n",
    "path\n",
    "practice\n",
    "concentration\n",
    "reason\n",
    "method\n",
    "ignorance\n",
    "anxiety\n",
    "wisdom'''.split('\\n')\n",
    "insight = list(set(insight))\n",
    "\n",
    "health='''health\n",
    "body\n",
    "suffering'''.split('\\n')\n",
    "health = list(set(health))\n",
    "\n",
    "community='''community\n",
    "people\n",
    "world\n",
    "elder\n",
    "relations'''.split('\\n')\n",
    "community = list(set(community))\n",
    "\n",
    "individual='''individual\n",
    "self\n",
    "subject\n",
    "person\n",
    "lives'''.split('\\n')\n",
    "individual = list(set(individual))\n",
    "\n",
    "time='''time\n",
    "state\n",
    "process\n",
    "activity'''.split('\\n')\n",
    "time = list(set(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats=[insight,health,community,individual,time]\n",
    "li=[]\n",
    "\n",
    "for cat in cats:\n",
    "    li.append(freq[freq['word'].isin(cat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Michal_Palinski/miniconda3/envs/myPython/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cats_str=['insight','health','community','individual','time']\n",
    "\n",
    "for no, cat in enumerate(cats_str):\n",
    "    li[no]['cat']=cat\n",
    "    \n",
    "df=pd.DataFrame(columns=['word','count','freq','cat'])\n",
    "df=df.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count']=df['count'].astype(int)\n",
    "# df['freq']=df['freq'].round(1)\n",
    "\n",
    "dfGr=df.groupby('cat').sum()\n",
    "dfGr['keywords']=df.groupby('cat').size()\n",
    "dfGr['count_per_keyword']=dfGr['count']/dfGr['keywords']\n",
    "dfGr['count_per_keyword']=dfGr['count_per_keyword'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "document_text = open('./texts/s_sutta.txt', 'r')\n",
    "text_string = document_text.read().lower()\n",
    "match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', text_string)\n",
    " \n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sum=sum(frequency.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGr['freq']=dfGr['count']/words_sum*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "      <th>keywords</th>\n",
       "      <th>count_per_keyword</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>community</th>\n",
       "      <td>2788</td>\n",
       "      <td>5.668625</td>\n",
       "      <td>5</td>\n",
       "      <td>557.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>3609</td>\n",
       "      <td>7.337901</td>\n",
       "      <td>3</td>\n",
       "      <td>1203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individual</th>\n",
       "      <td>4493</td>\n",
       "      <td>9.135270</td>\n",
       "      <td>5</td>\n",
       "      <td>898.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insight</th>\n",
       "      <td>15415</td>\n",
       "      <td>31.342130</td>\n",
       "      <td>19</td>\n",
       "      <td>811.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>2213</td>\n",
       "      <td>4.499522</td>\n",
       "      <td>4</td>\n",
       "      <td>553.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count       freq  keywords  count_per_keyword\n",
       "cat                                                      \n",
       "community    2788   5.668625         5              557.6\n",
       "health       3609   7.337901         3             1203.0\n",
       "individual   4493   9.135270         5              898.6\n",
       "insight     15415  31.342130        19              811.3\n",
       "time         2213   4.499522         4              553.2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfGr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=doc['text'].tolist()\n",
    "corpusStr=' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[x.lower() for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3712, 3196)\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(corpus)\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3712, 3196)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Michal_Palinski/miniconda3/envs/myPython/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:532: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Users/Michal_Palinski/miniconda3/envs/myPython/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:532: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=15, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep=pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=pyLDAvis.prepared_data_to_html(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    " pyLDAvis.save_html(prep,'./docs/lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "important=df['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0:00:00.183511\n",
      "wisdom\n",
      "reason\n",
      "path\n",
      "anxiety\n",
      "concentration\n",
      "reflection\n",
      "practice\n",
      "contemplating\n",
      "ignorance\n",
      "knowledge\n",
      "method\n",
      "feeling\n",
      "mental\n",
      "enlightenment\n",
      "comprehension\n",
      "insight\n",
      "consciousness\n",
      "mind\n",
      "mindfulness\n",
      "body\n",
      "health\n",
      "suffering\n",
      "community\n",
      "relations\n",
      "elder\n",
      "people\n",
      "world\n",
      "person\n",
      "individual\n",
      "subject\n",
      "lives\n",
      "self\n",
      "activity\n",
      "state\n",
      "time\n",
      "process\n",
      "coef wisdom 0.0 0:05:24.509058\n",
      "coef reason 2.7777777777777777 0:00:00.159586\n",
      "coef path 5.555555555555555 0:00:00.001589\n",
      "coef anxiety 8.333333333333334 0:00:00.001093\n",
      "coef concentration 11.11111111111111 0:00:00.000823\n",
      "coef reflection 13.88888888888889 0:00:00.001126\n",
      "coef practice 16.666666666666668 0:00:00.001123\n",
      "coef contemplating 19.444444444444443 0:00:00.001084\n",
      "coef ignorance 22.22222222222222 0:00:00.001203\n",
      "coef knowledge 25.0 0:00:00.001020\n",
      "coef method 27.77777777777778 0:00:00.000847\n",
      "coef feeling 30.555555555555557 0:00:00.000840\n",
      "coef mental 33.333333333333336 0:00:00.000980\n",
      "coef enlightenment 36.111111111111114 0:00:00.000943\n",
      "coef comprehension 38.888888888888886 0:00:00.000969\n",
      "coef insight 41.666666666666664 0:00:00.000923\n",
      "coef consciousness 44.44444444444444 0:00:00.001191\n",
      "coef mind 47.22222222222222 0:00:00.001098\n",
      "coef mindfulness 50.0 0:00:00.001115\n",
      "coef body 52.77777777777778 0:00:00.000911\n",
      "coef health 55.55555555555556 0:00:00.001008\n",
      "coef suffering 58.333333333333336 0:00:00.001021\n",
      "coef community 61.111111111111114 0:00:00.001006\n",
      "coef relations 63.888888888888886 0:00:00.001110\n",
      "coef elder 66.66666666666667 0:00:00.001018\n",
      "coef people 69.44444444444444 0:00:00.001020\n",
      "coef world 72.22222222222223 0:00:00.001170\n",
      "coef person 75.0 0:00:00.001337\n",
      "coef individual 77.77777777777777 0:00:00.001449\n",
      "coef subject 80.55555555555556 0:00:00.001505\n",
      "coef lives 83.33333333333333 0:00:00.001291\n",
      "coef self 86.11111111111111 0:00:00.001191\n",
      "coef activity 88.88888888888889 0:00:00.001184\n",
      "coef state 91.66666666666667 0:00:00.001168\n",
      "coef time 94.44444444444444 0:00:00.001121\n",
      "coef process 97.22222222222223 0:00:00.001450\n",
      "count wisdom 0.0 0:00:00.001138\n",
      "count reason 2.7777777777777777 0:00:02.370215\n",
      "count path 5.555555555555555 0:00:02.447866\n",
      "count anxiety 8.333333333333334 0:00:02.455371\n",
      "count concentration 11.11111111111111 0:00:02.369160\n",
      "count reflection 13.88888888888889 0:00:02.365717\n",
      "count practice 16.666666666666668 0:00:02.401530\n",
      "count contemplating 19.444444444444443 0:00:02.341215\n",
      "count ignorance 22.22222222222222 0:00:02.433036\n",
      "count knowledge 25.0 0:00:02.442219\n",
      "count method 27.77777777777778 0:00:02.377035\n",
      "count feeling 30.555555555555557 0:00:02.339568\n",
      "count mental 33.333333333333336 0:00:02.442110\n",
      "count enlightenment 36.111111111111114 0:00:02.362160\n",
      "count comprehension 38.888888888888886 0:00:02.353906\n",
      "count insight 41.666666666666664 0:00:02.474918\n",
      "count consciousness 44.44444444444444 0:00:02.466002\n",
      "count mind 47.22222222222222 0:00:02.366404\n",
      "count mindfulness 50.0 0:00:02.681637\n",
      "count body 52.77777777777778 0:00:02.379588\n",
      "count health 55.55555555555556 0:00:02.392072\n",
      "count suffering 58.333333333333336 0:00:02.485872\n",
      "count community 61.111111111111114 0:00:02.418066\n",
      "count relations 63.888888888888886 0:00:02.360095\n",
      "count elder 66.66666666666667 0:00:02.362198\n",
      "count people 69.44444444444444 0:00:02.438180\n",
      "count world 72.22222222222223 0:00:02.345899\n",
      "count person 75.0 0:00:02.422492\n",
      "count individual 77.77777777777777 0:00:02.488099\n",
      "count subject 80.55555555555556 0:00:02.668455\n",
      "count lives 83.33333333333333 0:00:03.025803\n",
      "count self 86.11111111111111 0:00:03.331595\n",
      "count activity 88.88888888888889 0:00:02.998678\n",
      "count state 91.66666666666667 0:00:02.816367\n",
      "count time 94.44444444444444 0:00:02.499326\n",
      "count process 97.22222222222223 0:00:02.725691\n"
     ]
    }
   ],
   "source": [
    "old_datetime = datetime.now()\n",
    "ngrams = [(1,1)]\n",
    "res = './res/sutta/'\n",
    "month = 'test'\n",
    "\n",
    "for ngram_art, ngram_compare in ngrams:\n",
    "#     dataframe, sort by coef\n",
    "#     take only these ngrams_compare which experience significant growth\n",
    "    coefs_compare = pd.read_csv(res + 'freq_' + str(ngram_compare) + '-all.csv', index_col=0)\n",
    "    coefs_compare = coefs_compare.loc[coefs_compare['freq'] < 0.5].sort_values('freq', ascending=False)\n",
    "    \n",
    "    coefs_art = pd.read_csv(res + 'freq_' + str(ngram_art) + '-all.csv', index_col=0)\n",
    "    coefs_art = coefs_art.loc[coefs_art['freq'] < 0.5].sort_values('freq', ascending=False)\n",
    "\n",
    "    # take only most common, growing ngrams\n",
    "#     important_words = coefs_art.index[2:12].tolist()\n",
    "    if ngram_art == 1:\n",
    "        important_words = important\n",
    "    elif ngram_art == 2:\n",
    "        important_words = important_2\n",
    "    \n",
    "    cooc_all = []\n",
    "    other_words = {}\n",
    "\n",
    "    # 5000 most significant ngrams\n",
    "    for i, row in coefs_compare[:2500].iterrows():\n",
    "        other_words[i] = row['freq']\n",
    "    \n",
    "    cooc = {}\n",
    "    print(month, datetime.now() - old_datetime)\n",
    "    old_datetime = datetime.now()\n",
    "        # use the fact that indices are the same\n",
    "        # probably more clear would be joining by links\n",
    "    \n",
    "    dfs_art = pd.read_csv(res + str(ngram_art) + '.csv')\n",
    "    dfs_art['text'] = dfs_art['text'].apply(lambda x: set(literal_eval(x)))\n",
    "    dfs_art.rename(columns={'text': 'text_token_art'}, inplace=True)\n",
    "    dfs_compare = pd.read_csv(res + str(ngram_compare) + '.csv')\n",
    "    dfs_compare['text'] = dfs_compare['text'].apply(lambda x: set(literal_eval(x)))\n",
    "    dfs_compare.rename(columns={'text': 'text_token_compare'}, inplace=True)\n",
    "        \n",
    "    dfs = pd.concat([dfs_art[['text_token_art']], dfs_compare[['text_token_compare']]], axis=1)\n",
    "        \n",
    "    article_word_count = {}\n",
    "        \n",
    "    # save RAM\n",
    "    del dfs_art\n",
    "    del dfs_compare\n",
    "    gc.collect()\n",
    "        \n",
    "    for word in important_words:\n",
    "        print(word)\n",
    "        cooc[word] = {}\n",
    "            # take only articles with the desired word\n",
    "        df_site_cont = dfs.loc[dfs['text_token_art'].apply(lambda x: word in x)]\n",
    "        article_word_count[word] = df_site_cont.shape[0]\n",
    "\n",
    "        # we don't want iterating a lot of times over a dataframe\n",
    "        # because it's slow, so first we assign zeroes to all words, and then add 1 if word exists\n",
    "        for k in other_words.keys():\n",
    "            cooc[word][k] = 0\n",
    "\n",
    "        # iterate over articles, then after words\n",
    "        for i, row in df_site_cont.iterrows():\n",
    "            for k in other_words.keys():\n",
    "                cooc[word][k] += k in row['text_token_compare']\n",
    "                \n",
    "        cooc_all.append(pd.DataFrame.from_dict(cooc))\n",
    "    \n",
    "    # concat\n",
    "    cooc = pd.concat(cooc_all, axis=1)\n",
    "\n",
    "    cooc = cooc.loc[:, ~cooc.columns.duplicated()]\n",
    "    # three coefficients\n",
    "    # frequency, count normalized by mean, count normalized by mean squared\n",
    "    for i_word, word in enumerate(important_words):\n",
    "        print('coef', word, 100*i_word/len(important_words), datetime.now() - old_datetime)\n",
    "        old_datetime = datetime.now()\n",
    "        if word not in cooc.columns:\n",
    "            continue\n",
    "        if article_word_count[word] > 0:\n",
    "            cooc[word + '_freq'] = 100 * cooc[word] / article_word_count[word]\n",
    "        else:\n",
    "            cooc[word + '_freq'] = 0\n",
    "                \n",
    "    # count weighted values\n",
    "    for i_word, word in enumerate(important_words):\n",
    "        print('count', word, 100*i_word/len(important_words), datetime.now() - old_datetime)\n",
    "        old_datetime = datetime.now()\n",
    "        cooc[word + '_art_' + 'freq_weighted'] = 0\n",
    "        cooc[word + '_art_' + 'freq_weighted_normalized'] = 0\n",
    "        cooc[word + '_art_' + 'freq_weighted_normalized2'] = 0\n",
    "        if word in cooc.columns: # avoid KeyErrors\n",
    "            cooc[word + '_art_' + 'freq_weighted'] += cooc[word + '_freq']\n",
    "            cooc[word + '_art_' + 'freq_weighted_normalized'] += cooc[word + '_freq']\n",
    "            cooc[word + '_art_' + 'freq_weighted_normalized2'] += cooc[word +  '_freq']\n",
    "        for word_compare, mean in other_words.items():\n",
    "            cooc.loc[word_compare, word + '_art_' + 'freq_weighted_normalized'] = cooc.loc[word_compare, word + '_art_' + 'freq_weighted_normalized'] / mean\n",
    "            cooc.loc[word_compare, word + '_art_' + 'freq_weighted_normalized2'] = cooc.loc[word_compare, word + '_art_' + 'freq_weighted_normalized2'] / mean**2\n",
    "\n",
    "    cooc[[x for x in cooc.columns if 'weighted' in x]].to_csv(res + 'cooc' + str(ngram_art) + str(ngram_compare) + 'weighted_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc=pd.read_csv('./res/sutta/cooc11weighted_2.csv')\n",
    "cooc.rename(columns={'Unnamed: 0':'keywords'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc.to_excel('suttas_cooc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "suff=cooc.sort_values('suffering_art_freq_weighted_normalized',ascending=False)[:30]['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=cooc.sort_values('body_art_freq_weighted_normalized',ascending=False)[:30]['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "peop=cooc.sort_values('people_art_freq_weighted_normalized',ascending=False)[:30]['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind=cooc.sort_values('mind_art_freq_weighted_normalized',ascending=False)[:30]['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs_chosen=pd.DataFrame(\n",
    "    {'temp':range(30),'suffering_s': suff,\n",
    "     'body_s': body,\n",
    "     'people_s':peop,\n",
    "     'mind_s':mind\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "coocs_chosen.to_csv('coocs_chosen_suttas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myPython]",
   "language": "python",
   "name": "conda-env-myPython-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
